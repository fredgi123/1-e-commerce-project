{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOurXByFeioKL81kGpLwKDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredgi123/1-e-commerce-project/blob/main/3_price_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n",
        "!pip install prophet\n",
        "!pip install statsmodels\n",
        "!pip install openpyxl\n",
        "!pip install keras\n",
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZjxnT84I5YV",
        "outputId": "ae79d67b-16f0-4510-f32b-deb4f8a63332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.10/dist-packages (1.1.6)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from prophet) (3.8.0)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.61)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from prophet) (4.66.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet) (6.4.5)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from holidays<1,>=0.25->prophet) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->prophet) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.4->prophet) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->holidays<1,>=0.25->prophet) (1.16.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.4)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.26.4)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.13.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-18SWOmIiSq",
        "outputId": "c3a5bdd2-3476-450f-855b-379189c36b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive successfully mounted.\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------\n",
        "# 1. Mount Google Drive\n",
        "# ----------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive successfully mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------\n",
        "# 2. Set Up Autoreload\n",
        "# ----------------------------------------\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "print(\"Autoreload extension loaded and set to mode 2.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.1. Import Standard Libraries\n",
        "# ----------------------------------------\n",
        "\n",
        "# --- System and Utilities ---\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import itertools\n",
        "import re\n",
        "import yaml\n",
        "import pickle\n",
        "import joblib\n",
        "import itertools\n",
        "import yaml\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pandas\")\n",
        "from joblib import load\n",
        "import importlib\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n",
        "\n",
        "# --- Data Handling ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "from plotly.subplots import make_subplots\n",
        "from PIL import Image as PILImage\n",
        "from IPython.display import Image as DisplayImage, display\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import seaborn as sns\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from matplotlib import style\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, InputLayer, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, Nadam\n",
        "\n",
        "# --- Scikit-learn ---\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# --- XGBoost ---\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Prophet ---\n",
        "from prophet import Prophet\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3. Define Paths and Add `scripts_p` to sys.path\n",
        "# ----------------------------------------\n",
        "\n",
        "# Define the path to your custom scripts\n",
        "scripts_p = '/content/drive/MyDrive/1_Pricing/scripts'\n",
        "\n",
        "# Add `scripts_p` to sys.path if not already added\n",
        "if scripts_p not in sys.path:\n",
        "    sys.path.append(scripts_p)\n",
        "    print(f'Added Scripts Path to sys.path: {scripts_p}')\n",
        "else:\n",
        "    print(f'Scripts path already in sys.path: {scripts_p}')\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4. Verify the Presence of `util_functions.py`\n",
        "# ----------------------------------------\n",
        "util_functions_path = os.path.join(scripts_p, \"util_functions.py\")\n",
        "if not os.path.exists(util_functions_path):\n",
        "    raise FileNotFoundError(f\"'util_functions.py' not found in {scripts_p}\")\n",
        "else:\n",
        "    print(f\"'util_functions.py' found at {util_functions_path}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5. Import Custom Modules Using importlib\n",
        "# ----------------------------------------\n",
        "import importlib.util\n",
        "\n",
        "def load_module(module_name, module_path):\n",
        "    \"\"\"\n",
        "    Load a module from a given file path using importlib.\n",
        "\n",
        "    Args:\n",
        "        module_name (str): The name to assign to the module.\n",
        "        module_path (str): The file path to the module.\n",
        "\n",
        "    Returns:\n",
        "        module: The loaded module object, or None if failed.\n",
        "    \"\"\"\n",
        "    spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
        "    if spec is None:\n",
        "        print(f\"Could not load spec for '{module_name}' at '{module_path}'.\")\n",
        "        return None\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    try:\n",
        "        spec.loader.exec_module(module)\n",
        "        print(f\"Module '{module_name}' loaded successfully.\")\n",
        "        return module\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load module '{module_name}': {e}\")\n",
        "        return None\n",
        "\n",
        "# Load all custom modules\n",
        "util_functions = load_module(\"util_functions\", util_functions_path)\n",
        "util_functions_synthetic_data = load_module(\"util_functions_synthetic_data\", os.path.join(scripts_p, \"util_functions_synthetic_data.py\"))\n",
        "data_preprocessing = load_module(\"data_preprocessing\", os.path.join(scripts_p, \"data_preprocessing.py\"))\n",
        "training_baseline = load_module(\"training_baseline\", os.path.join(scripts_p, \"training_baseline.py\"))\n",
        "training_lstm = load_module(\"training_lstm\", os.path.join(scripts_p, \"training_lstm.py\"))\n",
        "training_xgboost = load_module(\"training_xgboost\", os.path.join(scripts_p, \"training_xgboost.py\"))\n",
        "training_prophet = load_module(\"training_prophet\", os.path.join(scripts_p, \"training_prophet.py\"))\n",
        "visualizations = load_module(\"visualizations\", os.path.join(scripts_p, \"visualizations.py\"))\n",
        "predict = load_module(\"predict\", os.path.join(scripts_p, \"predict.py\"))\n",
        "\n",
        "# Check if all modules are loaded\n",
        "modules = [util_functions, util_functions_synthetic_data, data_preprocessing,\n",
        "           training_baseline, training_lstm, training_xgboost, training_prophet, visualizations, predict]\n",
        "for module in modules:\n",
        "    if module is None:\n",
        "        print(\"One or more modules failed to load. Please check the errors above.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6. Define and Assign Directory Paths\n",
        "# ----------------------------------------\n",
        "\n",
        "# Define directory structure\n",
        "directory_structure = {\n",
        "    \"data\": {\n",
        "        \"synthetic_data\": {},\n",
        "        \"raw_data\": {},\n",
        "        \"intermediate_data\": {}\n",
        "    },\n",
        "    \"models\": {\n",
        "        \"temp\": {},\n",
        "        \"prophet\": {},\n",
        "        \"lstm\": {},\n",
        "        \"xgboost\": {},\n",
        "        \"scalers\":{}\n",
        "    },\n",
        "    \"notebooks\": {},\n",
        "    \"config\": {},        # Added config directory\n",
        "    \"dict\": {},\n",
        "    \"predictions\": {},\n",
        "    \"results\": {},\n",
        "    \"scripts\": {},\n",
        "    \"env\": {},\n",
        "    \"static\": {\n",
        "        \"images\": {},\n",
        "        \"videos\": {}\n",
        "    },\n",
        "    \"temp\": {}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "default_dict_p = '/content/drive/MyDrive/1_Pricing/dict'\n",
        "\n",
        "# Use the `create_paths` function from `util_functions_synthetic_data` module\n",
        "if util_functions_synthetic_data and hasattr(util_functions_synthetic_data, 'create_paths'):\n",
        "    loaded_paths = util_functions_synthetic_data.create_paths(default_dict_p, directory_structure, imports=False)\n",
        "    print(\"Paths created successfully.\")\n",
        "else:\n",
        "    raise AttributeError(\"util_functions_synthetic_data module does not have 'create_paths' function or is not loaded.\")\n",
        "\n",
        "# Assign directory paths\n",
        "dict_p = loaded_paths.get('dict_p')\n",
        "images_p = loaded_paths.get('images_p')\n",
        "raw_data_p = loaded_paths.get('raw_data_p')\n",
        "results_p = loaded_paths.get('results_p')\n",
        "synthetic_data_p = loaded_paths.get('synthetic_data_p')\n",
        "scripts_p = loaded_paths.get('scripts_p')\n",
        "predictions_p = loaded_paths.get('predictions_p')\n",
        "models_p = loaded_paths.get('models_p')\n",
        "static_p = loaded_paths.get('static_p')\n",
        "videos_p = loaded_paths.get('videos_p')\n",
        "env_p = loaded_paths.get('env_p')\n",
        "notebooks_p = loaded_paths.get('notebooks_p')\n",
        "intermediate_p = loaded_paths.get('intermediate_data_p')\n",
        "temp_p = loaded_paths.get('temp_p')\n",
        "prophet_p = loaded_paths.get('prophet_p')\n",
        "lstm_p = loaded_paths.get('lstm_p')\n",
        "xgboost_p = loaded_paths.get('xgboost_p')\n",
        "config_p = loaded_paths.get('config_p')\n",
        "scalers_p = loaded_paths.get('scalers_p')\n",
        "\n",
        "categorie = \"soap\"\n",
        "categorie_path = os.path.join(synthetic_data_p, categorie)\n",
        "loaded_paths['categorie_p'] = categorie_path\n",
        "\n",
        "print(\"Paths assigned successfully.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7. Restore the Original Working Directory\n",
        "# ----------------------------------------\n",
        "original_cwd = '/content/drive/MyDrive/1_Pricing/notebooks'  # Ensure this is your intended directory\n",
        "os.chdir(original_cwd)\n",
        "print(f\"Working directory restored to: {os.getcwd()}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8. Utilize Imported Modules\n",
        "# ----------------------------------------\n",
        "\n",
        "# Example usage of util_functions\n",
        "if util_functions:\n",
        "    print(\"\\nFunctions available in 'util_functions':\")\n",
        "    print(dir(util_functions))  # List available functions and attributes\n",
        "\n",
        "    # Example: Using a function from util_functions\n",
        "    if hasattr(util_functions, 'save_config'):\n",
        "        print(\"\\n'save_config' function is available.\")\n",
        "        # Example usage:\n",
        "        # config = {'key': 'value'}\n",
        "        # util_functions.save_config(config, 'config.yaml')\n",
        "    else:\n",
        "        print(\"\\n'save_config' function is NOT available in 'util_functions'.\")\n",
        "else:\n",
        "    print(\"util_functions module is not loaded.\")\n",
        "from data_preprocessing import *\n",
        "from training_baseline import *\n",
        "from training_xgboost import *\n",
        "from training_prophet import *\n",
        "from visualizations import *\n",
        "from predict import *\n",
        "from util_functions_training import *\n",
        "from util_functions import *\n",
        "from predict_xgboost import *\n",
        "from pipeline_preparation import *\n",
        "from training_lstm import*\n",
        "from util_functions import apply_lag_transfo_local_indices, assign_product_index, feature_product_indices\n",
        "print(categorie_path)\n",
        "from lstm_preprocessing import (\n",
        "    calculate_max_lag,\n",
        "    validate_test_len,\n",
        "    slice_indices,\n",
        "    apply_lagged_features_df,\n",
        "    generate_lstm_sequences,\n",
        "    train_split,\n",
        "    log_transform_and_scale,\n",
        "    lstm_pipeline_with_slicing,\n",
        "    execute_pipeline,\n",
        "    explore_combinations\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "DfVZSr35L_Jh",
        "outputId": "b2f54b50-bf11-421b-d5c7-c1ff4f50f18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Autoreload extension loaded and set to mode 2.\n",
            "Scripts path already in sys.path: /content/drive/MyDrive/1_Pricing/scripts\n",
            "'util_functions.py' found at /content/drive/MyDrive/1_Pricing/scripts/util_functions.py\n",
            "Module 'util_functions' loaded successfully.\n",
            "Module 'util_functions_synthetic_data' loaded successfully.\n",
            "Module 'data_preprocessing' loaded successfully.\n",
            "Module 'training_baseline' loaded successfully.\n",
            "Module 'training_lstm' loaded successfully.\n",
            "Module 'training_xgboost' loaded successfully.\n",
            "Module 'training_prophet' loaded successfully.\n",
            "Module 'visualizations' loaded successfully.\n",
            "Module 'predict' loaded successfully.\n",
            "Paths created successfully.\n",
            "Paths assigned successfully.\n",
            "Working directory restored to: /content/drive/MyDrive/1_Pricing/notebooks\n",
            "\n",
            "Functions available in 'util_functions':\n",
            "['MinMaxScaler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'apply_lag_transfo_local_indices', 'assign_product_index', 'create_paths', 'feature_product_indices', 'get_profit', 'inverse_transform', 'is_gpu_available', 'json', 'load', 'load_array', 'load_df', 'load_unit_cost', 'log_transform_and_scale', 'mean_squared_error', 'np', 'os', 'pd', 'pickle', 'random', 'random_unit_cost', 'transform_scale_known_scaler', 'yaml']\n",
            "\n",
            "'save_config' function is NOT available in 'util_functions'.\n",
            "/content/drive/MyDrive/1_Pricing/data/synthetic_data/soap\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'lstm_preprocessing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-42a184fd2526>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_lag_transfo_local_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massign_product_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_product_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorie_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m from lstm_preprocessing import (\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcalculate_max_lag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mvalidate_test_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lstm_preprocessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions already present in util_function.py\n",
        "def feature_product_indices(feature_list, num_products, predictors, p_index):\n",
        "    feature_indices = [feature_list.index(feature) for feature in predictors]\n",
        "    return [(feature_index * num_products) + product_index\n",
        "            for feature_index in feature_indices for product_index in p_index]\n",
        "\n",
        "def assign_product_index(param, num_products):\n",
        "    indices_dict = {\n",
        "        \"all_idx\": list(range(num_products)),\n",
        "        \"comp_idx\": list(range(num_products // 2, num_products)),\n",
        "        \"our_idx\": list(range(num_products // 2)),\n",
        "    }\n",
        "    return indices_dict.get(param, ValueError(f\"Invalid parameter name: {param}\"))\n",
        "\n",
        "class GeneralConfig:\n",
        "    def __init__(self, config_file, directory_structure, categorie=\"soap\"):\n",
        "\n",
        "        # Load configuration\n",
        "        self.paths = directory_structure\n",
        "        self.categorie = categorie\n",
        "        self.synthetic_data_path = os.path.join(directory_structure['synthetic_data_p'], categorie)\n",
        "        self.categorie_path = directory_structure['categorie_p']\n",
        "        self.config_p = directory_structure[\"config_p\"]\n",
        "        self.prophet_p = directory_structure[\"prophet_p\"]\n",
        "        self.scalers_p = directory_structure[\"scalers_p\"]  # Adding config path to the class\n",
        "\n",
        "        with open(\"{}/{}\".format(self.config_p,config_file), \"r\") as file:\n",
        "            self.config = yaml.safe_load(file)\n",
        "\n",
        "        # General parameters\n",
        "        self.num_products = self.config['general']['num_products']\n",
        "        self.test_len = self.config['general']['test_len']\n",
        "        self.prediction_steps = self.config['general']['prediction_steps']\n",
        "        self.validation_split_ratio = self.config['general']['validation_split_ratio']\n",
        "        self.all_features = self.config['general']['all_features']\n",
        "\n",
        "        self.categorie = categorie\n",
        "\n",
        "         # Initialize full data\n",
        "        self.x_arr_2d  = None\n",
        "        self.y_arr_2d = None\n",
        "        self.full_df = None\n",
        "        self.full_arr2d_deseas = None\n",
        "        self.df_full_deseas = None\n",
        "        self.initialize_full_data()\n",
        "\n",
        "    def initialize_full_data(self):\n",
        "        _, _, arr_2d, idx, _ = load_array(self.categorie_path, self.categorie, self.all_features, self.num_products)\n",
        "        self.num_weeks = len(idx)\n",
        "        self.col_names = [f\"{f}_p{i}\" for f in self.all_features for i in range(self.num_products)]\n",
        "        self.idx = idx.copy()\n",
        "        self.date_idx = pd.date_range(start = idx[0], periods = self.num_weeks, freq=\"W\")\n",
        "        self.x_arr_2d =arr_2d.copy()\n",
        "        self.y_arr_2d = arr_2d.copy()\n",
        "        self.full_df = pd.DataFrame(self.y_arr_2d, columns=self.col_names, index=idx)\n",
        "\n",
        "    def load_lstm_config(self, model_name, model_config_key):\n",
        "        \"\"\"\n",
        "        Dynamically load LSTM configuration parameters and calculate indices.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model (e.g., \"m1_lstm\").\n",
        "            model_config_key (str): Key in the configuration file for the model (e.g., \"MODEL_1_LSTM\").\n",
        "        \"\"\"\n",
        "        config_filename = f\"{model_name}_config.yml\"\n",
        "        config_path = os.path.join(self.config_p, config_filename)\n",
        "\n",
        "        # Load LSTM configuration\n",
        "        with open(config_path, \"r\") as file:\n",
        "            model_config = yaml.safe_load(file).get(model_config_key, {})\n",
        "\n",
        "        # Dynamically set attributes for all parameters\n",
        "        for section, params in model_config.items():\n",
        "            for key, value in params.items():\n",
        "                setattr(self, f\"{model_name}_{key}\", value)\n",
        "                print(f\"Set attribute {model_name}_{key} = {value}\")\n",
        "\n",
        "        # Assign and calculate indices for X and Y\n",
        "        for var in ['x', 'y']:\n",
        "            product_indices_attr = f\"{model_name}_{var}_product_indices\"\n",
        "            feature_attr = f\"{model_name}_{var}_predictors\" if var == 'x' else f\"{model_name}_y_target\"\n",
        "            # Assign product indices\n",
        "            indices = assign_product_index(getattr(self, product_indices_attr), self.num_products)\n",
        "            setattr(self, product_indices_attr, indices)\n",
        "            # Calculate feature-specific indices\n",
        "            setattr(self, f\"{model_name}_{var}_indices\",feature_product_indices(self.all_features, self.num_products, getattr(self, feature_attr), indices),)\n",
        "        # calculate and assign local_lag indices (for slicing array and apply lags)\n",
        "        setattr(self, f\"{model_name}_local_lag_indices\", apply_lag_transfo_local_indices(getattr(self, f\"{model_name}_x_apply_lag\"),self.all_features,\n",
        "                                                                                         getattr(self, f\"{model_name}_x_product_indices\"),),)\n",
        "        print(f\"{model_name}: All indices and parameters loaded successfully.\")\n",
        "\n",
        "    def get_lstm_config(self, model_name):\n",
        "        \"\"\" example     get_lstm_config(geral, \"m42\")  after geral = GeneralConfig( and geral.load_lstm_config(model_name=\"m42\", model_config_key=\"MODEL_1_LSTM\")\n",
        "        Returns:\n",
        "            dict: Dictionary containing LSTM-specific parameters.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"x_predictors\": getattr(self, f\"{model_name}_x_predictors\"),\n",
        "            \"y_target\": getattr(self, f\"{model_name}_y_target\"),\n",
        "            \"x_product_indices\": getattr(self, f\"{model_name}_x_product_indices\"),\n",
        "            \"y_product_indices\": getattr(self, f\"{model_name}_y_product_indices\"),\n",
        "            \"x_apply_lag\": getattr(self, f\"{model_name}_x_apply_lag\"),\n",
        "            \"back_windows\": getattr(self, f\"{model_name}_back_windows\"),\n",
        "            \"prediction_steps\": getattr(self, f\"{model_name}_prediction_steps\"),\n",
        "            \"offset\": getattr(self, f\"{model_name}_offset\"),\n",
        "            \"test_len\": self.test_len,\n",
        "            \"validation_len\": self.test_len,\n",
        "        }\n",
        "    def get_feature_array(self, feature):\n",
        "        feature_idx = np.arange(self.num_products) + self.num_products * self.all_features.index(feature)\n",
        "        return  self.x_arr_2d[:, feature_idx]\n",
        "\n",
        "    def get_sliced_array(self, feature, product_indices):\n",
        "        \"\"\"Slice the feature array for specific product indices.\"\"\"\n",
        "        feature_idx = np.arange(self.num_products) + self.num_products * self.all_features.index(feature)\n",
        "        return self.x_arr_2d[:, feature_idx][:, product_indices]\n",
        "\n",
        "    def get_prophet_array(self):\n",
        "        \"\"\"Load or train Prophet models and deseasonalize demand data.\"\"\"\n",
        "        self.full_arr2d_deseas, self.df_full_deseas, self.baseline_w_trend, self.models_list = load_prophet_array(\n",
        "            data  = self.x_arr_2d, idx = self.date_idx, category  = self.categorie, model_p = self.paths['prophet_p'], all_features  = self.all_features,\n",
        "            num_products  = self.num_products, importation =  True, model_file_name =  \"prophet_model.pkl\")\n",
        "\n",
        "# Initialize GeneralConfig\n",
        "geral = GeneralConfig(config_file=\"general_config.yml\", directory_structure=loaded_paths, categorie=\"soap\")\n"
      ],
      "metadata": {
        "id": "kLTmSk0ZK8ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Lstm pre_processing pipeline"
      ],
      "metadata": {
        "id": "I8JyXSVfRTRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LzY9yHJOZ_XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "geral.load_lstm_config(model_name=\"m42\", model_config_key=\"MODEL_1_LSTM\")\n",
        "x_predictors_combinations = [[\"price\", \"demand\"], [\"price\", \"mktg\", \"demand\"]]\n",
        "combination_results = execute_pipeline(geral, x_predictors_combinations, model_name=\"m42\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "B94Me9C8chmU",
        "outputId": "958481e8-75ca-43f4-edc1-1c813556b031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set attribute m42_num_lstm_layers = [2, 3]\n",
            "Set attribute m42_num_units = [128, 256]\n",
            "Set attribute m42_dropout_rate = [0.05, 0.1, 0.2]\n",
            "Set attribute m42_recurrent_dropout = [0.05, 0.1]\n",
            "Set attribute m42_optimizer = ['nadam', 'adam']\n",
            "Set attribute m42_learning_rate = [0.001, 0.0005]\n",
            "Set attribute m42_epochs = [75, 150]\n",
            "Set attribute m42_batch_size = [32, 16]\n",
            "Set attribute m42_back_windows = [4, 8, 12, 16, 20]\n",
            "Set attribute m42_prediction_steps = [4, 6]\n",
            "Set attribute m42_log_transform = [True]\n",
            "Set attribute m42_deseasonalize = [True, False]\n",
            "Set attribute m42_x_feature_range = [[-1, 1]]\n",
            "Set attribute m42_y_feature_range = [[0, 1]]\n",
            "Set attribute m42_offset = 0\n",
            "Set attribute m42_x_features_to_offset = []\n",
            "Set attribute m42_y_features_to_offset = []\n",
            "Set attribute m42_x_predictors = ['demand', 'price', 'mktg']\n",
            "Set attribute m42_y_target = ['demand']\n",
            "Set attribute m42_x_apply_lag = {'price': [0, 1, 2], 'display': [0], 'distribution': [0], 'mktg': [0, 1], 'demand': [0, 1, 2, 3, 4]}\n",
            "Set attribute m42_x_product_indices = all_idx\n",
            "Set attribute m42_y_product_indices = all_idx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-49d89c8cf89a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgeral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lstm_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"m42\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MODEL_1_LSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_predictors_combinations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"demand\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"price\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mktg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"demand\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcombination_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_predictors_combinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"m42\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-141-64f17895bb8d>\u001b[0m in \u001b[0;36mload_lstm_config\u001b[0;34m(self, model_name, model_config_key)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_indices_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Calculate feature-specific indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{model_name}_{var}_indices\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_product_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_products\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# calculate and assign local_lag indices (for slicing array and apply lags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         setattr(self, f\"{model_name}_local_lag_indices\", apply_lag_transfo_local_indices(getattr(self, f\"{model_name}_x_apply_lag\"),self.all_features, \n",
            "\u001b[0;32m/content/drive/MyDrive/1_Pricing/scripts/util_functions.py\u001b[0m in \u001b[0;36mfeature_product_indices\u001b[0;34m(all_features, predictors, p_index, num_products)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_product_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_products\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mfeature_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_index\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_products\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproduct_index\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_indices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mproduct_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_combinations(combination_results):\n",
        "    count = 0\n",
        "    for combin_name, combin_result in combination_results.items():\n",
        "        print(f\"\\nCombination: {combin_name}\")\n",
        "        for key, value in combin_result.items():\n",
        "            count += 1\n",
        "            if key == 'results_array':  # Specifically check for 'results_array'\n",
        "                print(f\"  {key}:\")\n",
        "                # Iterate over back windows in the results_array\n",
        "                for back_window, split in value.items():\n",
        "                    print(f\"    Back Window: {back_window}\")\n",
        "                    count += 1\n",
        "                    # Optionally print the shape of sequences in the split dictionary\n",
        "                    for split_key, split_value in split.items():\n",
        "                        print(f\"      {split_key}: Shape {split_value.shape}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")\n",
        "    return count\n",
        "count = explore_combinations(combination_results)\n",
        "print(\"total combinations\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qNNsCbTOKnS",
        "outputId": "e68c1284-8160-4d96-b42a-215a9cc0bee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combination: m42_combination_1\n",
            "  taxonomy_name: m42_config_m42_combination_1\n",
            "  x_predictors: ['price', 'demand']\n",
            "  log_transform: True\n",
            "  deseasonalize: True\n",
            "  x_feature_range: (-1, 1)\n",
            "  y_feature_range: (0, 1)\n",
            "  apply_lag: {'price': [0, 1, 2], 'display': [0], 'distribution': [0], 'mktg': [0, 1], 'demand': [0, 1, 2, 3, 4]}\n",
            "  X_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/X_scaler_m42_combination_1.save\n",
            "  Y_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/Y_scaler_m42_combination_1.save\n",
            "  results_array:\n",
            "    Back Window: 4\n",
            "      X_train_seq: Shape (94, 4, 80)\n",
            "      Y_train_seq: Shape (94, 4, 10)\n",
            "      X_val_seq: Shape (15, 4, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 4, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 8\n",
            "      X_train_seq: Shape (91, 8, 80)\n",
            "      Y_train_seq: Shape (91, 4, 10)\n",
            "      X_val_seq: Shape (15, 8, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 8, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 12\n",
            "      X_train_seq: Shape (89, 12, 80)\n",
            "      Y_train_seq: Shape (89, 4, 10)\n",
            "      X_val_seq: Shape (15, 12, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 12, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 16\n",
            "      X_train_seq: Shape (86, 16, 80)\n",
            "      Y_train_seq: Shape (86, 4, 10)\n",
            "      X_val_seq: Shape (15, 16, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 16, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 20\n",
            "      X_train_seq: Shape (83, 20, 80)\n",
            "      Y_train_seq: Shape (83, 4, 10)\n",
            "      X_val_seq: Shape (15, 20, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 20, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "\n",
            "Combination: m42_combination_2\n",
            "  taxonomy_name: m42_config_m42_combination_2\n",
            "  x_predictors: ['price', 'mktg', 'demand']\n",
            "  log_transform: True\n",
            "  deseasonalize: True\n",
            "  x_feature_range: (-1, 1)\n",
            "  y_feature_range: (0, 1)\n",
            "  apply_lag: {'price': [0, 1, 2], 'display': [0], 'distribution': [0], 'mktg': [0, 1], 'demand': [0, 1, 2, 3, 4]}\n",
            "  X_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/X_scaler_m42_combination_2.save\n",
            "  Y_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/Y_scaler_m42_combination_2.save\n",
            "  results_array:\n",
            "    Back Window: 4\n",
            "      X_train_seq: Shape (94, 4, 100)\n",
            "      Y_train_seq: Shape (94, 4, 10)\n",
            "      X_val_seq: Shape (15, 4, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 4, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 8\n",
            "      X_train_seq: Shape (91, 8, 100)\n",
            "      Y_train_seq: Shape (91, 4, 10)\n",
            "      X_val_seq: Shape (15, 8, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 8, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 12\n",
            "      X_train_seq: Shape (89, 12, 100)\n",
            "      Y_train_seq: Shape (89, 4, 10)\n",
            "      X_val_seq: Shape (15, 12, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 12, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 16\n",
            "      X_train_seq: Shape (86, 16, 100)\n",
            "      Y_train_seq: Shape (86, 4, 10)\n",
            "      X_val_seq: Shape (15, 16, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 16, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 20\n",
            "      X_train_seq: Shape (83, 20, 100)\n",
            "      Y_train_seq: Shape (83, 4, 10)\n",
            "      X_val_seq: Shape (15, 20, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 20, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "\n",
            "Combination: m42_combination_3\n",
            "  taxonomy_name: m42_config_m42_combination_3\n",
            "  x_predictors: ['price', 'demand']\n",
            "  log_transform: True\n",
            "  deseasonalize: False\n",
            "  x_feature_range: (-1, 1)\n",
            "  y_feature_range: (0, 1)\n",
            "  apply_lag: {'price': [0, 1, 2], 'display': [0], 'distribution': [0], 'mktg': [0, 1], 'demand': [0, 1, 2, 3, 4]}\n",
            "  X_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/X_scaler_m42_combination_3.save\n",
            "  Y_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/Y_scaler_m42_combination_3.save\n",
            "  results_array:\n",
            "    Back Window: 4\n",
            "      X_train_seq: Shape (94, 4, 80)\n",
            "      Y_train_seq: Shape (94, 4, 10)\n",
            "      X_val_seq: Shape (15, 4, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 4, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 8\n",
            "      X_train_seq: Shape (91, 8, 80)\n",
            "      Y_train_seq: Shape (91, 4, 10)\n",
            "      X_val_seq: Shape (15, 8, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 8, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 12\n",
            "      X_train_seq: Shape (89, 12, 80)\n",
            "      Y_train_seq: Shape (89, 4, 10)\n",
            "      X_val_seq: Shape (15, 12, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 12, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 16\n",
            "      X_train_seq: Shape (86, 16, 80)\n",
            "      Y_train_seq: Shape (86, 4, 10)\n",
            "      X_val_seq: Shape (15, 16, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 16, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 20\n",
            "      X_train_seq: Shape (83, 20, 80)\n",
            "      Y_train_seq: Shape (83, 4, 10)\n",
            "      X_val_seq: Shape (15, 20, 80)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 20, 80)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "\n",
            "Combination: m42_combination_4\n",
            "  taxonomy_name: m42_config_m42_combination_4\n",
            "  x_predictors: ['price', 'mktg', 'demand']\n",
            "  log_transform: True\n",
            "  deseasonalize: False\n",
            "  x_feature_range: (-1, 1)\n",
            "  y_feature_range: (0, 1)\n",
            "  apply_lag: {'price': [0, 1, 2], 'display': [0], 'distribution': [0], 'mktg': [0, 1], 'demand': [0, 1, 2, 3, 4]}\n",
            "  X_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/X_scaler_m42_combination_4.save\n",
            "  Y_scaler_path: /content/drive/MyDrive/1_Pricing/models/scalers/Y_scaler_m42_combination_4.save\n",
            "  results_array:\n",
            "    Back Window: 4\n",
            "      X_train_seq: Shape (94, 4, 100)\n",
            "      Y_train_seq: Shape (94, 4, 10)\n",
            "      X_val_seq: Shape (15, 4, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 4, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 8\n",
            "      X_train_seq: Shape (91, 8, 100)\n",
            "      Y_train_seq: Shape (91, 4, 10)\n",
            "      X_val_seq: Shape (15, 8, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 8, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 12\n",
            "      X_train_seq: Shape (89, 12, 100)\n",
            "      Y_train_seq: Shape (89, 4, 10)\n",
            "      X_val_seq: Shape (15, 12, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 12, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 16\n",
            "      X_train_seq: Shape (86, 16, 100)\n",
            "      Y_train_seq: Shape (86, 4, 10)\n",
            "      X_val_seq: Shape (15, 16, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 16, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "    Back Window: 20\n",
            "      X_train_seq: Shape (83, 20, 100)\n",
            "      Y_train_seq: Shape (83, 4, 10)\n",
            "      X_val_seq: Shape (15, 20, 100)\n",
            "      Y_val_seq: Shape (15, 4, 10)\n",
            "      X_test_seq: Shape (15, 20, 100)\n",
            "      Y_test_seq: Shape (15, 4, 10)\n",
            "total combinations 60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from itertools import product\n",
        "\n",
        "# --- Data Manipulation ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Model Training and Metrics ---\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "# Step 1: Calculate max_lag dynamically based on x_apply_lag\n",
        "def calculate_max_lag(x_apply_lag):\n",
        "    \"\"\"Calculate the maximum lag required for features in x_apply_lag.\"\"\"\n",
        "    max_lag = max([max(lags) for lags in x_apply_lag.values()])\n",
        "    return max_lag\n",
        "\n",
        "\n",
        "# Step 2: Validate test_len calculation\n",
        "def validate_test_len(back_window, prediction_steps, offset, x_apply_lag):\n",
        "    \"\"\"\n",
        "    Validate and calculate test_len based on the parameters and x_apply_lag.\n",
        "    \"\"\"\n",
        "    max_lag = calculate_max_lag(x_apply_lag)\n",
        "    test_len = back_window + prediction_steps + offset + max_lag\n",
        "    print(f\"Calculated max_lag: {max_lag}\")\n",
        "    print(f\"Calculated test_len: {test_len}\")\n",
        "    return test_len\n",
        "\n",
        "\n",
        "# Step 3: Slice the indices based on predictors and targets\n",
        "def slice_indices(X_offset, Y_offset, x_predictors, y_target, x_product_indices, y_product_indices):\n",
        "    col_names_x = [f\"{feature}_p{product_idx}\" for feature in x_predictors for product_idx in x_product_indices]\n",
        "    col_names_y = [f\"{feature}_p{product_idx}\" for feature in y_target for product_idx in y_product_indices]\n",
        "\n",
        "    sliced_X = X_offset[col_names_x]\n",
        "    sliced_Y = Y_offset[col_names_y]\n",
        "\n",
        "    print(\"Sliced X shape:\", sliced_X.shape)\n",
        "    print(\"Sliced Y shape:\", sliced_Y.shape)\n",
        "\n",
        "    return sliced_X, sliced_Y\n",
        "\n",
        "\n",
        "# Step 4: Apply lagged features\n",
        "def apply_lagged_features_df(df, x_apply_lag):\n",
        "    \"\"\"Generate lagged features based on x_apply_lag.\"\"\"\n",
        "    max_lag = max(max(lags) for lags in x_apply_lag.values())\n",
        "    lagged_cols = {\n",
        "        f\"{col}_lag{lag}\": df[col].shift(lag)\n",
        "        for feature, lags in x_apply_lag.items()\n",
        "        for col in df.columns if col.startswith(f\"{feature}_\")\n",
        "        for lag in lags\n",
        "    }\n",
        "    lagged_df = pd.DataFrame(lagged_cols, index=df.index).iloc[max_lag:].reset_index(drop=True)\n",
        "    print(f\"apply_lagged_features_df outputs: lagged_df shape: {lagged_df.shape}, max_lag: {max_lag}\")\n",
        "    return lagged_df, max_lag\n",
        "\n",
        "\n",
        "# Step 5: Generate LSTM sequences\n",
        "def generate_lstm_sequences(X, Y, back_window_size, prediction_step_size, offset):\n",
        "    \"\"\"Generate LSTM sequences.\"\"\"\n",
        "    total_samples = len(X)\n",
        "    assert total_samples > back_window_size + prediction_step_size, \"Insufficient samples for sequence generation.\"\n",
        "\n",
        "    X_sequences, Y_sequences = [], []\n",
        "    for i in range(total_samples - back_window_size - prediction_step_size + 1):\n",
        "        X_seq = X[i + offset: i + back_window_size + offset]\n",
        "        Y_seq = Y[i + back_window_size: i + back_window_size + prediction_step_size]\n",
        "        X_sequences.append(X_seq)\n",
        "        Y_sequences.append(Y_seq)\n",
        "\n",
        "    return np.array(X_sequences), np.array(Y_sequences)\n",
        "\n",
        "\n",
        "# Step 6: Train/validation/test split\n",
        "def train_split(back_window, X_sequences, Y_sequences, test_len, validation_len, overlap_authorized=False):\n",
        "    \"\"\"Split sequences into training, validation, and test sets.\"\"\"\n",
        "    total_samples = len(X_sequences)\n",
        "    if not overlap_authorized:\n",
        "        train_end_idx = total_samples - test_len - validation_len\n",
        "        val_end_idx = total_samples - test_len\n",
        "\n",
        "        return {\n",
        "            \"X_train_seq\": X_sequences[:train_end_idx],\n",
        "            \"Y_train_seq\": Y_sequences[:train_end_idx],\n",
        "            \"X_val_seq\": X_sequences[train_end_idx:val_end_idx],\n",
        "            \"Y_val_seq\": Y_sequences[train_end_idx:val_end_idx],\n",
        "            \"X_test_seq\": X_sequences[val_end_idx:],\n",
        "            \"Y_test_seq\": Y_sequences[val_end_idx:],\n",
        "        }\n",
        "    else:\n",
        "        train_end_idx = int(total_samples * 0.65)\n",
        "        val_end_idx = int(total_samples * 0.85)\n",
        "        val_start_idx = val_end_idx - validation_len + int(0.1 * validation_len)\n",
        "        test_start_idx = -validation_len + int(0.1 * validation_len)\n",
        "        return {\n",
        "            \"X_train_seq\": X_sequences[:train_end_idx],\n",
        "            \"Y_train_seq\": Y_sequences[:train_end_idx],\n",
        "            \"X_val_seq\": X_sequences[val_start_idx:val_end_idx],\n",
        "            \"Y_val_seq\": Y_sequences[val_start_idx:val_end_idx],\n",
        "            \"X_test_seq\": X_sequences[test_start_idx:],\n",
        "            \"Y_test_seq\": Y_sequences[test_start_idx:],\n",
        "        }\n",
        "def log_transform_and_scale(df, log_transform, x_feature_range, y_feature_range, taxonomy_name, saving_path):\n",
        "    os.makedirs(saving_path, exist_ok=True)  # Ensure saving path exists\n",
        "\n",
        "    X_scaler = MinMaxScaler(feature_range=x_feature_range)\n",
        "    Y_scaler = MinMaxScaler(feature_range=y_feature_range)\n",
        "\n",
        "    df_x = df.copy()\n",
        "    df_y = df.copy()\n",
        "    cols = df.columns  # Maintain column names\n",
        "\n",
        "    # Apply log transformation if specified\n",
        "    if log_transform:\n",
        "        print(\"Applying log transformation to predictors and targets...\")\n",
        "        df_x = np.log1p(df_x)\n",
        "        df_y = np.log1p(df_y)\n",
        "\n",
        "    # Fit and transform using MinMaxScaler\n",
        "    df_x = pd.DataFrame(X_scaler.fit_transform(df_x), columns=cols, index=df.index)\n",
        "    df_y = pd.DataFrame(Y_scaler.fit_transform(df_y), columns=cols, index=df.index)\n",
        "\n",
        "    # Save scalers for future use\n",
        "    from joblib import dump\n",
        "    X_scaler_path = os.path.join(saving_path, f\"X_scaler_{taxonomy_name}.save\")\n",
        "    Y_scaler_path = os.path.join(saving_path, f\"Y_scaler_{taxonomy_name}.save\")\n",
        "    dump(X_scaler, X_scaler_path)\n",
        "    dump(Y_scaler, Y_scaler_path)\n",
        "\n",
        "    print(f\"Scalers saved to {X_scaler_path} and {Y_scaler_path}\")\n",
        "\n",
        "    return df_x, df_y, df_x.values, df_y.values, X_scaler_path, Y_scaler_path\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T1Lg3YT34EM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_models_for_combinations(results, baseline_hyperparams, prediction_steps, model_path, taxonomy_prefix):\n",
        "    updated_results = {}\n",
        "\n",
        "    for combin_name, combin_result in results.items():\n",
        "        print(f\"\\nTraining for {combin_name}...\")\n",
        "\n",
        "        # Construct taxonomy name\n",
        "        taxonomy_name = f\"{taxonomy_prefix}_{combin_name}\"\n",
        "\n",
        "        if \"results_array\" not in combin_result:\n",
        "            raise KeyError(f\"results_array missing for {combin_name}. Available keys: {list(combin_result.keys())}\")\n",
        "\n",
        "        results_array = combin_result[\"results_array\"]\n",
        "        if not results_array:\n",
        "            raise ValueError(f\"results_array is empty or None for {combin_name}\")\n",
        "\n",
        "        # Train the model\n",
        "        m4_best_model, m4_best_params = train_lstm_prediction_model_with_preprocessed_arrays(\n",
        "            results =                   results_array,\n",
        "            baseline_hyperparams =      baseline_hyperparams,\n",
        "            prediction_steps =          prediction_steps,\n",
        "            model_path =                lstm_p,\n",
        "            taxonomy_name =             taxonomy_name,\n",
        "            importation =               True  # Set to False to train the model\n",
        "        )\n",
        "\n",
        "        # Store the best model and parameters in the updated results\n",
        "        updated_results[combin_name] = {\n",
        "            **combin_result,  # Include all existing combination details\n",
        "            \"best_model\": m4_best_model,\n",
        "            \"best_params\": m4_best_params\n",
        "        }\n",
        "\n",
        "        # Log the best parameters\n",
        "        print(f\"Best parameters for {combin_name}: {m4_best_params}\")\n",
        "\n",
        "    return updated_results\n",
        "\n",
        "# Call the function to train models for all combinations\n",
        "phase_2_results = train_models_for_combinations(\n",
        "    results =                       combination_results,  # The results dictionary from the previous pipeline\n",
        "    baseline_hyperparams =          lstm_hyperparams_phase2,  # Baseline hyperparameter HERE\n",
        "    prediction_steps =              4,  # here should be general config in class\n",
        "    model_path =                    lstm_p,  # here should be general config\n",
        "    taxonomy_prefix =               \"m4_demand_lstm_phase2\"\n",
        "    importation = True  # Taxonomy prefix\n",
        ")\n",
        "print(geral.m4_lstm_hyperparams_phase2)"
      ],
      "metadata": {
        "id": "8aFDF8QBfHHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm_prediction_model_with_preprocessed_arrays(\n",
        "    results,\n",
        "    baseline_hyperparams,\n",
        "    prediction_steps,\n",
        "    model_path=\"lstm_models\",\n",
        "    taxonomy_name=\"lstm_baseline\",\n",
        "    importation=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Train or load an LSTM model using preprocessed sequences.\n",
        "\n",
        "    Parameters:\n",
        "        - results (dict): Dictionary containing sequences for each back window.\n",
        "        - baseline_hyperparams (dict): Hyperparameter grid for the model.\n",
        "        - prediction_steps (int): Number of prediction steps for the model.\n",
        "        - model_path (str): Directory path to save or load the model.\n",
        "        - taxonomy_name (str): Unique name for saving/loading the model.\n",
        "        - importation (bool): If True, loads an existing model instead of training.\n",
        "\n",
        "    Returns:\n",
        "        - best_model (tf.keras.Model): Trained or loaded LSTM model.\n",
        "        - best_params (dict): Dictionary of best hyperparameters and metrics.\n",
        "    \"\"\"\n",
        "    if not model_path:\n",
        "        raise ValueError(\"Model path is None or invalid.\")\n",
        "    if not taxonomy_name:\n",
        "        raise ValueError(\"Taxonomy name is None or invalid.\")\n",
        "\n",
        "    # Ensure the model directory exists\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "    # Handle importation of pre-trained model\n",
        "    if importation:\n",
        "        best_model_path = os.path.join(model_path, f\"{taxonomy_name}_best_model.keras\")\n",
        "        best_params_path = os.path.join(model_path, f\"{taxonomy_name}_best_params.json\")\n",
        "\n",
        "        if not os.path.exists(best_model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {best_model_path}\")\n",
        "\n",
        "        # Load model and parameters\n",
        "        best_model = tf.keras.models.load_model(best_model_path)\n",
        "        with open(best_params_path, 'r') as f:\n",
        "            best_params = json.load(f)\n",
        "        print(f\"Loaded model from {best_model_path}\")\n",
        "        print(f\"Loaded parameters from {best_params_path}\")\n",
        "        return best_model, best_params\n",
        "\n",
        "    # Initialize training variables\n",
        "    best_model = None\n",
        "    best_score = float('inf')\n",
        "    best_params = {}\n",
        "    all_scores = {}\n",
        "\n",
        "    # Early stopping and learning rate scheduler\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6)\n",
        "\n",
        "    # Iterate over back windows\n",
        "    for back_window, split in results.items():\n",
        "        print(f\"\\nTraining for Back Window: {back_window}\")\n",
        "        X_train_seq = split['X_train_seq']\n",
        "        Y_train_seq = split['Y_train_seq']\n",
        "        X_val_seq = split['X_val_seq']\n",
        "        Y_val_seq = split['Y_val_seq']\n",
        "\n",
        "        num_features = X_train_seq.shape[2]\n",
        "        num_targets = Y_train_seq.shape[2]\n",
        "\n",
        "        # Generate all hyperparameter combinations\n",
        "        hyperparams_keys = list(baseline_hyperparams.keys())\n",
        "        hyperparams_values = list(baseline_hyperparams.values())\n",
        "        hyperparams_combinations = list(itertools.product(*hyperparams_values))\n",
        "\n",
        "        for idx, combination in enumerate(hyperparams_combinations, 1):\n",
        "            params = dict(zip(hyperparams_keys, combination))\n",
        "            print(f\"Training combination {idx}/{len(hyperparams_combinations)}: {params}\")\n",
        "\n",
        "            # Build and compile the LSTM model\n",
        "            model = build_lstm_model(\n",
        "                params,\n",
        "                back_window_size=back_window,\n",
        "                num_features=num_features,\n",
        "                num_targets=num_targets,\n",
        "                prediction_step_size=prediction_steps\n",
        "            )\n",
        "\n",
        "            # Define model checkpoint path\n",
        "            checkpoint_path = os.path.join(model_path, f\"{taxonomy_name}_temp_model.keras\")\n",
        "            checkpoint = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "            # Train the model\n",
        "            history = model.fit(\n",
        "                X_train_seq, Y_train_seq,\n",
        "                validation_data=(X_val_seq, Y_val_seq),\n",
        "                epochs=params['epochs'],\n",
        "                batch_size=params['batch_size'],\n",
        "                callbacks=[early_stop, lr_scheduler, checkpoint],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Evaluate the model\n",
        "            Y_val_pred = model.predict(X_val_seq)\n",
        "            Y_val_seq_flat = Y_val_seq.reshape(-1, Y_val_seq.shape[-1])\n",
        "            Y_val_pred_flat = Y_val_pred.reshape(-1, Y_val_pred.shape[-1])\n",
        "\n",
        "            mse = mean_squared_error(Y_val_seq_flat, Y_val_pred_flat)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(Y_val_seq_flat, Y_val_pred_flat)\n",
        "            r2 = r2_score(Y_val_seq_flat, Y_val_pred_flat)\n",
        "\n",
        "            # Compute percentage-based metrics\n",
        "            epsilon = 1e-10\n",
        "            denominator = np.abs(Y_val_seq_flat)\n",
        "            denominator = np.where(denominator < epsilon, epsilon, denominator)\n",
        "            safe_mape = np.mean(np.abs((Y_val_seq_flat - Y_val_pred_flat) / denominator)) * 100\n",
        "            smape = np.mean(2 * np.abs(Y_val_seq_flat - Y_val_pred_flat) / (np.abs(Y_val_seq_flat) + np.abs(Y_val_pred_flat) + epsilon)) * 100\n",
        "\n",
        "            # Update the best model\n",
        "            if mse < best_score:\n",
        "                best_score = mse\n",
        "                all_scores = {\n",
        "                    \"mse\": mse,\n",
        "                    \"rmse\": rmse,\n",
        "                    \"mae\": mae,\n",
        "                    \"r2\": r2,\n",
        "                    \"safe_mape\": safe_mape,\n",
        "                    \"smape\": smape\n",
        "                }\n",
        "                best_model = model\n",
        "                best_params = params.copy()\n",
        "                best_params.update({\n",
        "                    'taxonomy': taxonomy_name,\n",
        "                    'back_window': back_window,\n",
        "                    'prediction_steps': prediction_steps,\n",
        "                    'num_features': num_features,\n",
        "                    'num_targets': num_targets,\n",
        "                    'best_score': best_score,\n",
        "                    'all_scores': all_scores\n",
        "                })\n",
        "                print(f\"New best model for back_window={back_window} with MSE={mse:.4f}\")\n",
        "\n",
        "    # Save the best model and parameters\n",
        "    best_model_path = os.path.join(model_path, f\"{taxonomy_name}_best_model.keras\")\n",
        "    best_params_path = os.path.join(model_path, f\"{taxonomy_name}_best_params.json\")\n",
        "\n",
        "    best_model.save(best_model_path)\n",
        "    with open(best_params_path, 'w') as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "    print(f\"\\nBest model saved to {best_model_path}\")\n",
        "    print(f\"Best parameters saved to {best_params_path}\")\n",
        "\n",
        "    return best_model, best_params\n",
        "\n"
      ],
      "metadata": {
        "id": "iq80ez1VfLcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bRoO_P6Xa4KD"
      }
    }
  ]
}